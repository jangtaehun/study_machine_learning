{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a623fbf-b4f5-441a-8489-1c018debbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "\n",
    "    f1 = f1_score(y_test, pred)\n",
    "\n",
    "    # ROC-AUC 추가\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n",
    "\n",
    "def precision_recall_curve_plot(y_test=None, pred_proba_c1=None):\n",
    "\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
    "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
    "\n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "\n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "\n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21419e57-92ef-457a-b0c4-b5edcc27a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "0    73012\n",
      "1     3008\n",
      "Name: count, dtype: int64\n",
      "unsatisfied 비율은 3.96\n",
      "santander customer satisfaction: 데이터 세트 Null 값 갯수  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "var3\n",
       " 2         74165\n",
       " 8           138\n",
       "-999999      116\n",
       " 9           110\n",
       " 3           108\n",
       "           ...  \n",
       " 231           1\n",
       " 188           1\n",
       " 168           1\n",
       " 135           1\n",
       " 87            1\n",
       "Name: count, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cust_df = pd.read_csv(\"../../../data/santander-customer-satisfaction/train.csv\", encoding='latin-1')\n",
    "predict_df = pd.read_csv(\"../../../data/santander-customer-satisfaction/test.csv\", encoding='latin-1')\n",
    "santander_submission_df = pd.read_csv(\"../../../data/santander-customer-satisfaction/sample_submission.csv\", encoding='latin-1')\n",
    "\n",
    "print(cust_df['TARGET'].value_counts())\n",
    "\n",
    "total_cnt = cust_df.TARGET.count()\n",
    "unsatisfied_cnt = cust_df[cust_df['TARGET'] == 1].TARGET.count()\n",
    "print('unsatisfied 비율은 {0:.2f}'.format((unsatisfied_cnt / total_cnt * 100)))\n",
    "\n",
    "print('santander customer satisfaction: 데이터 세트 Null 값 갯수 ',cust_df.isnull().sum().sum())\n",
    "\n",
    "cust_df.drop('ID', axis=1, inplace=True) # 필요 없는 행 삭제\n",
    "cust_df['var3'].value_counts() # 이상치 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadcbe15-cba0-4c51-b50b-0c18f7e6221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var3\n",
       "2.000000      74165\n",
       "8.000000        138\n",
       "2.717577        116\n",
       "9.000000        110\n",
       "3.000000        108\n",
       "              ...  \n",
       "231.000000        1\n",
       "188.000000        1\n",
       "168.000000        1\n",
       "135.000000        1\n",
       "87.000000         1\n",
       "Name: count, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # var3 피처 값 대체(2가 많으니 2로 대체) 및 ID 피처 드롭 -> 고민이 필요함\n",
    "# most_frequent_value = cust_df['var3'].value_counts().idxmax()\n",
    "# cust_df['var3'].replace(-999999, most_frequent_value, inplace=True)\n",
    "\n",
    "\n",
    "# 평균값 계산 (이 경우 -999999 값을 제외하고 계산)\n",
    "mean_value = cust_df[cust_df['var3'] != -999999]['var3'].mean()\n",
    "cust_df['var3'].replace(-999999, mean_value, inplace=True)\n",
    "\n",
    "\n",
    "# # 중앙값 계산 (이 경우 -999999 값을 제외하고 계산)\n",
    "# median_value = cust_df[cust_df['var3'] != -999999]['var3'].median()\n",
    "# cust_df['var3'].replace(-999999, median_value, inplace=True)\n",
    "\n",
    "\n",
    "# # -999999 값을 고유한 값 (예: 0)으로 대체\n",
    "# cust_df['var3'].replace(-999999, 0, inplace=True)\n",
    "\n",
    "\n",
    "# -999999 값을 가진 행 제거\n",
    "# cust_df = cust_df[cust_df['var3'] != -999999]\n",
    "\n",
    "\n",
    "# KNN Imputation: K-최근접 이웃 알고리즘을 사용하여 결측값을 대체\n",
    "# 결측값을 K-최근접 이웃의 평균값으로 대체하며, 모델 기반 대체는 다른 변수들을 사용하여 결측값을 예측\n",
    "\n",
    "\n",
    "# # -999999 값을 NaN으로 대체\n",
    "# from sklearn.impute import KNNImputer\n",
    "# cust_df['var3'].replace(-999999, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# # KNN Imputer 생성 및 적용\n",
    "# imputer = KNNImputer(n_neighbors=2)\n",
    "# df_imputed = imputer.fit_transform(cust_df)\n",
    "\n",
    "# # 결과를 데이터프레임으로 변환\n",
    "# cust_df = pd.DataFrame(df_imputed, columns=cust_df.columns)\n",
    "\n",
    "\n",
    "cust_df['var3'].value_counts() # 이상치 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673829f5-332f-4ef5-93c7-542d056da6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리\n",
    "X = cust_df.iloc[:, :-1]\n",
    "y = cust_df.iloc[:, -1]\n",
    "print('피처 데이터 shape:{0}'.format(X.shape))\n",
    "print('피처 데이터 shape:{0}'.format(y.shape))\n",
    "print('santander customer satisfaction: 데이터 세트 Null 값 갯수 ', X.isnull().sum().sum())\n",
    "\n",
    "# 표준화 => 2차원 데이터\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "print('피처 데이터 shape:{0}'.format(X.shape))\n",
    "\n",
    "# predict_df에서의 ID 드롭\n",
    "predict_df.drop('ID', axis=1, inplace=True)\n",
    "predict_df = sc.transform(predict_df)\n",
    "print('피처 데이터 shape:{0}'.format(predict_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f0918-e460-4e85-bf38-5907a8d50576",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f94f0-5154-497f-9286-c1ebff11f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "smote = SMOTE()\n",
    "X_1, y_1 = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size=0.2, random_state=0)\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape), '\\n')\n",
    "\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt * 100)\n",
    "\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt * 100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f5fbe-14bb-43f9-a7a4-1fd743497440",
   "metadata": {},
   "source": [
    "# ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b9df2-8036-41f7-b83d-9edc6fdd18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adasyn = ADASYN()\n",
    "X_2, y_2 = adasyn.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=0)\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape), '\\n')\n",
    "\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt * 100)\n",
    "\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt * 100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc794bcd-5017-4a53-ac46-ce81d08fbd91",
   "metadata": {},
   "source": [
    "# SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef393b18-9f50-4b4c-9dc2-0cfeaf3a4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "smoteto = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "X_3, y_3 = smoteto.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=0)\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape), '\\n')\n",
    "\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt * 100)\n",
    "\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt * 100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12130b02-f5b1-4b03-8646-6e1e61f434e8",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68b832-6eaa-437d-80cc-acb9b9677ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA를 실행하여 설명된 분산 비율을 구하기\n",
    "pca = PCA()\n",
    "pca1 = pca.fit(X_3)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# 누적 설명된 분산 비율을 구하기\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# 누적 설명된 분산 비율이 90% 이상이 되는 지점을 찾기\n",
    "threshold = 0.90\n",
    "optimal_n_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "\n",
    "# 설명된 분산 비율을 시각화\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "plt.axvline(x=optimal_n_components, color='r', linestyle='-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of components for 90% variance: {optimal_n_components}\")\n",
    "\n",
    "# 설명된 분산 비율이 90% 또는 95% 이상을 차지하는 지점에서의 n_components 값을 선택할 수 있다.\n",
    "# 일반적으로 누적 설명된 분산 비율이 90% 이상이 되는 지점을 선택하는 것이 좋다.\n",
    "# 설명된 분산 비율이 급격히 증가하다가 점차 완만해지는 \"엘보(elbow)\" 지점을 찾는 것이 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b78399-844c-410e-84b2-8f57229754d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=optimal_n_components)\n",
    "pca_test = pca.fit_transform(X_3)\n",
    "predict_df = pca.transform(predict_df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_test, y_3, test_size=0.2, random_state=0)\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape , X_test.shape), '\\n')\n",
    "\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt * 100)\n",
    "\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt * 100, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1692da5-931e-4504-b443-c48625f04c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271c321-b735-432e-9a29-528b27ae8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ecbf1-5bcb-47a6-9cf7-b041cf020242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63987b2-ef8c-45c1-9aa6-0dfe567a4dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
